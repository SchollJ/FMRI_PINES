{"title":"Bids Conversion","markdown":{"yaml":{"title":"Bids Conversion","author":"Anastasios Dadiotis","editor":"visual","toc":true,"format":{"html":{"code-tools":true,"self-contained":true}}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\nThis document is a step-by-step guide on how to convert raw data from a study to [BIDS](https://bids.neuroimaging.io/) format.[^1] The tool that is used here is [HeuDiConv](https://neuroimaging-core-docs.readthedocs.io/en/latest/pages/heudiconv.html). The data used in this example is from the Tiger study and already downloaded from xnat (see - add link when right this thing). The data are in a dicom format. Running HeuDiConv is a 3 step procedure. Note that this procedure takes place to the crnl cluster so this code is to be used with the cluster terminal and slurm. Most of the code is written in bash, but there are some python scripts that needs to be modified manually for the Bids conversion.\n\n[^1]: For an excellent tutorial for Bids conversion [see](https://sarenseeley.github.io/BIDS-fmriprep-MRIQC.html).\n\n# Bids Conversion\n\n## Setup\n\nIn this preliminary step we setup and define our variables. Note from the code below, change the variable \"subject\" to the subject you are working on. The Tiger notation is C for controls and G for Gamblers, e.g. for the first control subject the notation is C01 and for the first gambler G01. The following procedure is to submit the job for one subject. (to be updated for more).\n\n``` bash\n# Standard variable for the Tiger study\nWD=/crnldata/psyr2/Anastasios/Tiger_fmri\nmy_study=Tiger\nsession_name=S01\n\n# change this to the subject you are working on\nsubject=XXX \n```\n\n## [Step 1](https://neuroimaging-core-docs.readthedocs.io/en/latest/pages/heudiconv.html#heudiconv-step1): Generate a heuristic.py file.\n\nBy passing some path information and flags to HeuDiConv, you generate a heuristic (translation) file skeleton and some associated descriptor text files. These all get placed in a **hidden** directory, .heudiconv under the bids/derivatives directory.\n\n``` bash\ncd ${WD}/${my_study}/code\nsbatch step1_heudiconv.sh ${subject} ${session_name} ${WD} ${my_study} \nsqueue\n```\n\nTo check that it worked the out_step1\\_\\*.log file should have finished with exit code 0. Also, go to the /crnldata/psyr2/Anastasios/Tiger_fmri/Tiger/data/bids/derivatives/.heudiconv/{subject}/info were 5 files should be created.\n\nNote that this is a **hidden** directory. To see it you need to type `ls -a` in the terminal. The file that we need for the next step is the dicominfo.tsv file.\n\nThis step takes around 15 to 20 minutes to finish.\n\n## [Step 2](https://neuroimaging-core-docs.readthedocs.io/en/latest/pages/heudiconv.html#heudiconv-step2): Modify the heuristic.py file\n\nYou will modify the heuristic.py to specify BIDS output names and directories, and the input DICOM characteristics. Available input DICOM characteristics are listed in /.heudiconv/info/dicominfo.tsv.\n\nCheck the template of the heuristic.py file specifically for the Tiger study in the following path: /crnldata/psyr2/Anastasios/Tiger_fmri/Tiger/code/heuristic.py. This is the one you should modify based on the dicominfo.tsv file of the specific subject.\n\nThe template is almost ready. What you should modify are the following lines:\n\n1.  Line 119 s.dim == XXXX\n2.  Line 123 s.dim == XXXX\n3.  Line 127 s.dim == XXXX\n\nThe rest should remain the shame. At the end of this page there is the the heuristic.py file for inspection if nececarry.\n\n## [Step 3](https://neuroimaging-core-docs.readthedocs.io/en/latest/pages/heudiconv.html#heudiconv-step3): Run HeuDiConv\n\nNow that the heuristic.py file is ready we can run the HeuDiConv. Each time you run it, additional subdirectories are created under .heudiconv that record the details of each subject (and session) conversion. Detailed provenance information is retained in the .heudiconv hidden directory. The following code is to be run in the terminal.\n\n``` bash\ncd ${WD}/${my_study}/code\nchmod -R 770 .\nsbatch step3_heudiconv.sh ${subject} ${session_name} ${WD} ${my_study}\nsqueue\n```\n\n# Bids Validation\n\nAfter the conversion is done, it is important to validate the Bids format. This is done by using the [BIDS validator](https://bids-standard.github.io/bids-validator/).\n\n# Scripts\n\n## step1_heudiconv.sh\n\n``` {.bash filename=\"step1_heudiconv.sh\" code-line-numbers=\"true\" code-fold=\"true\"}\n#!/bin/bash\n\n###############################\n### Original script from Arnaud Fournel, PhD, NeuroPop team, CRNL, Lyon\n### arnaud.fournel @ inserm.fr\n### \n### Adapted for the CRNL study\n### by Gaelle Leroux, PhD\n### and Isabelle Faillenot, PhD\n###\n### Autumn 2020, Lyon\n### gaelle.leroux @ cnrs.fr\n###\n### launched by: sbatch step1_heudiconv.sh ${subject} ${session_name} ${WD} ${my_study}\n###\n###############################\n#\n### The SBATCH directives (line 39 to be revised only):\n\n### Your job name displayed by the queue\n### use \"squeue\" command in a terminal to see it\n#SBATCH --job-name=HeuDC_1\n\n### Specify output and error files\n### %A for job array's master job allocation number\n### or %a for job array ID (index) number\n#SBATCH --output=out_step1_%A.log\n#SBATCH --error=err_step1_%A.log\n\n### Specify the number of tasks, CPU per task and buffer size to be used \n### (up to 4 CPU/task to reach optimal power)\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=10G\n\n### The size of the participant table to be run\n###SBATCH --array=1-1\n\n### If you want to launch your script on a specific node of the cluser\n### If yes, uncomment the appropirate line(s) \n###SBATCH --exclude=node9\n###SBATCH --nodelist=node10\n\n# End of the SBATCH directives\n###############################\n#\n# Paths of the study\nsubject=$1\nsession_name=$2\nWD=$3\nmy_study=$4\nSTUDY=${WD}/${my_study}\nDATA_DIRECTORY=\"${STUDY}/data\"\n#\nHEUDICONV_SINGULARITY_IMG=\"/mnt/data/soft/Images/heudiconv_1.1.0.sif\"\n#\n## Comment: Parse the participants.tsv file and extract one subject ID from the line corresponding to this SLURM task.\n## Comment: 1 line below to uncomment if you want to launch the script for all the subjects listed in ${DATA_DIRECTORY}/participants.tsv\n#subject=$( sed -n -E \"$((${SLURM_ARRAY_TASK_ID} + 1))s/sub-(\\S*)\\>.*/\\1/gp\" ${DATA_DIRECTORY}/participants.tsv )\n#\n# To be printed in the out_*.log file :\necho \"#########################################################################\"\necho \"User:\" $USER\necho \"#\"\necho \"SLURM_SUBMITING_DIRECTORY:\" $SLURM_SUBMIT_DIR\necho \"SLURM_JOB_NODELIST:\" $SLURM_NODELIST\necho \"SLURM_JOB_NAME:\" $SLURM_JOB_NAME\necho \"SLURM_JOB_ID:\" $SLURM_JOBID\necho \"SLURM_ARRAY_TASK_ID:\" $SLURM_ARRAY_TASK_ID\necho \"SLURM_NTASKS:\" $SLURM_NTASKS\necho \"#\"\necho \"Step 1: generation of text files using HeuDiConv\"\necho \"#\"\necho \"Subject processed:\" ${subject}\necho \"Session processed:\" ${session_name}\necho \"#\"\necho \"Job STARTED @ $(date)\"\necho \"#\"\n#\n# STEP 1/3: generate heuristic file based on a template file + 4 other text files\n# Submission to slurm and running the HeuDiConv singularity image\n#\n# Check the \"-d\" path (line 83) pointing at the dicom files\n#\n# Compose the command line\ncmd=\"srun singularity run \\\n    --cleanenv \\\n    -B ${DATA_DIRECTORY}:/base \\\n    ${HEUDICONV_SINGULARITY_IMG} \\\n    -d /base/dicom/{subject}/{session}/scans/*/resources/DICOM/files/*.??? \\\n    -s ${subject} \\\n    --ses ${session_name} \\\n    -f convertall \\\n    -c none \\\n    -o /base/bids/derivatives \\\n    --overwrite\"\n#\n# Setup done, run the command\necho Running task ${SLURM_ARRAY_TASK_ID}\necho \"Command line used (example of the last subject processed)\"\necho $cmd\n#\necho \"#\"\n#\neval $cmd\nexitcode=$?\n#\necho \"#\"\necho \"Job STOPPED @ $(date)\"\necho \"#\"\necho \"If STEP 1 successful: 5 text files were generated in\" ${STUDY}\"/data/bids/derivatives/.heudiconv/\"$1\"/info\"\necho \"#\"\necho \"Now, STEP 2: edit the empty file heuristitic.py just created and copy/paste it to\" ${STUDY}\"/code\"\necho \"#\"\necho \"An example of heuristic file for a study@primage is provided in\" ${STUDY}\"/code/heuristic_templates/heuristic.py\"\necho \"#########################################################################\"\necho \"#\"\n# For your information about step 2:\n# The heuristic file controls how information about the dicoms is used to convert to a file system layout (e.g., BIDS). \n# This is a python file that must have the function infotodict, which takes a single argument seqinfo.\n#\n# Output results to a table\necho \"sub-$subject  ${SLURM_ARRAY_TASK_ID} $exitcode\" >> ${SLURM_JOB_NAME}.step1.${SLURM_ARRAY_JOB_ID}.tsv\necho Finished tasks ${SLURM_ARRAY_TASK_ID} with exit code $exitcode\nexit $exitcode\n```\n\n## heuristic.py\n\n``` {.python filename=\"heuristic.py\" code-line-numbers=\"true\" code-fold=\"true\"}\nfrom __future__ import annotations\n\nimport logging\nfrom typing import Optional\n\nfrom heudiconv.utils import SeqInfo\n\nlgr = logging.getLogger(\"heudiconv\")\n\n\ndef create_key(\n    template: Optional[str],\n    outtype: tuple[str, ...] = (\"nii.gz\",),\n    annotation_classes: None = None,\n) -> tuple[str, tuple[str, ...], None]:\n    if template is None or not template:\n        raise ValueError(\"Template must be a valid format string\")\n    return (template, outtype, annotation_classes)\n\n\ndef infotodict(\n    seqinfo: list[SeqInfo],\n) -> dict[tuple[str, tuple[str, ...], None], list[str]]:\n    \"\"\"Heuristic evaluator for determining which runs belong where\n\n    allowed template fields - follow python string module:\n\n    item: index within category\n    subject: participant id\n    seqitem: run number during scanning\n    subindex: sub index within group\n    \"\"\"\n\n    # \"data\" creates sequential numbers which can be for naming sequences.\n    # This is especially valuable if you run the same sequence multiple times at the scanner.\n    data = create_key('run-{item:03d}')\n\n    # Anatomical images\n    # Structural scans (anat specification): MUST end with \"T1w\" or \"T2w\" or \"FLAIR\" or \"T1map\"...\n    # list: https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/01-magnetic-resonance-imaging-data.html#anatomy-imaging-data\n    t1w = create_key('sub-{subject}/{session}/anat/sub-{subject}_{session}_T1w')\n\n    # Functional images\n    # Tasks, including movies (func specification): MUST contain \"task-\" in the name + \"bold\" or \"sbref\" or \"cbv\" or \"phase\" at the end\n    # list: https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/01-magnetic-resonance-imaging-data.html#task-including-resting-state-imaging-data\n    # For tasks and movies, we need to specify the task name in the file name and then all the tasks are in the func folder. \n\n    # Big Buck Bunny task\n    boldBigBuckBunny = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-BigBuckBunny_bold')\n    boldBigBuckBunny_sbref = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-BigBuckBunny_sbref')\n\n    # Hariri task\n    boldHariri = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-Hariri_bold')\n    boldHariri_sbref = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-Hariri_sbref')\n\n    # Partly Cloudy task\n    boldPartlyCloudy = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-PartlyCloudy_bold')\n    boldPartlyCloudy_sbref = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-PartlyCloudy_sbref')   \n\n    # Skewed gambling task NOTE: this task has multiple runs\n    boldSkewedGambling = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-SkewedGambling_run-{item:02d}_bold')\n    boldSkewedGambling_sbref = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-SkewedGambling_run-{item:02d}_sbref')\n\n\n    # field maps (fmap specification): the file name must end with \"magnitude\" or \"phasediff\" and include the {subject}\n    # specifications: https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/01-magnetic-resonance-imaging-data.html#fieldmap-data\n    fmap_magn = create_key('sub-{subject}/{session}/fmap/sub-{subject}_{session}_magnitude')\n    fmap_phase = create_key('sub-{subject}/{session}/fmap/sub-{subject}_{session}_phasediff')\n\n    # Diffusion scans (dwi speccification): MUST end with \"dwi\"\n    # specifications: https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/01-magnetic-resonance-imaging-data.html#diffusion-imaging-data\n    dwi = create_key('sub-{subject}/{session}/dwi/sub-{subject}_{session}_acq-{acq}_dir-{dir}_dwi')\n    dwi_sbref = create_key('sub-{subject}/{session}/dwi/sub-{subject}_{session}_acq-{acq}_dir-{dir}_sbref')\n\n\n    info: dict[tuple[str, tuple[str, ...], None], list[str]] = {data: [],\n                                                                boldHariri: [],\n                                                                boldHariri_sbref: [],\n                                                                boldBigBuckBunny: [],\n                                                                boldBigBuckBunny_sbref: [],\n                                                                boldPartlyCloudy: [],\n                                                                boldPartlyCloudy_sbref: [],\n                                                                boldSkewedGambling: [],\n                                                                boldSkewedGambling_sbref: [],\n                                                                fmap_magn: [],\n                                                                fmap_phase: [],\n                                                                t1w: [],\n                                                                dwi: [],\n                                                                dwi_sbref: []\n                                                                }\n\n    for s in seqinfo:\n        \"\"\"\n        The namedtuple `s` contains the following fields:\n\n        * total_files_till_now\n        * example_dcm_file\n        * series_id\n        * dcm_dir_name\n        * unspecified2\n        * unspecified3\n        * dim1\n        * dim2\n        * dim3\n        * dim4\n        * TR\n        * TE\n        * protocol_name\n        * is_motion_corrected\n        * is_derived\n        * patient_id\n        * study_description\n        * referring_physician_name\n        * series_description\n        * image_type\n        \"\"\"\n        if (\"emotion\" in s.protocol_name and s.dim4 == 3) :\n            info[boldHariri_sbref].append(s.series_id)\n        if (\"emotion\" in s.protocol_name and s.dim4 == 1257) :\n            info[boldHariri].append(s.series_id)\n        if (\"film_1\" in s.protocol_name and s.dim4 == 3) :\n            info[boldBigBuckBunny_sbref].append(s.series_id)\n        if (\"film_1\" in s.protocol_name and s.dim4 == 999) :\n            info[boldBigBuckBunny].append(s.series_id)\n        if (\"film_2\" in s.protocol_name and s.dim4 == 3) :\n            info[boldPartlyCloudy_sbref].append(s.series_id)\n        if (\"film_2\" in s.protocol_name and s.dim4 == 726) :\n            info[boldPartlyCloudy].append(s.series_id)\n        if (\"gambling\" in s.protocol_name and s.dim4 == 3) :\n            info[boldSkewedGambling_sbref].append(s.series_id)\n        if (\"gambling\" in s.protocol_name and s.dim4 > 1400 ) :\n            info[boldSkewedGambling].append(s.series_id)\n        if (\"gre_field_mapping\" in s.protocol_name and s.dim4 == 106):\n            info[fmap_magn].append(s.series_id)\n        if (\"gre_field_mapping\" in s.protocol_name and s.dim4 == 53):\n            info[fmap_phase].append(s.series_id)\n        if (\"T1_SAG\" in s.protocol_name) and ('NORM' in s.image_type) :\n            info[t1w].append(s.series_id)\n        if (\"dmri\" in s.protocol_name and \"b2700\" in s.series_description and \"PA\" in s.series_description and s.dim4 == 1):\n            info[dwi_sbref].append({\"item\": s.series_id, \"acq\": \"b2700\", 'dir': \"PA\"})\n        if (\"dmri\" in s.protocol_name and \"b2700\" in s.series_description and \"PA\" in s.series_description and s.dim4 == 151):\n            info[dwi].append({\"item\": s.series_id, \"acq\": \"b2700\", 'dir': \"PA\"})\n        if (\"dmri\" in s.protocol_name and \"b2700\" in s.series_description and \"AP\" in s.series_description and s.dim4 == 1):\n            info[dwi_sbref].append({\"item\": s.series_id, \"acq\": \"b2700\", 'dir': \"AP\"})\n        if (\"dmri\" in s.protocol_name and \"b2700\" in s.series_description and \"AP\" in s.series_description and s.dim4 == 151):\n            info[dwi].append({\"item\": s.series_id, \"acq\": \"b2700\", 'dir': \"AP\"})\n        if (\"dmri\" in s.protocol_name and \"b0\" in s.series_description and \"AP\" in s.series_description and s.dim4 == 1):\n            info[dwi_sbref].append({\"item\": s.series_id, \"acq\": \"b0\", 'dir': \"AP\"})\n        if (\"dmri\" in s.protocol_name and \"b0\" in s.series_description and \"AP\" in s.series_description and s.dim4 == 7):\n            info[dwi].append({\"item\": s.series_id, \"acq\": \"b0\", 'dir': \"AP\"})\n        if (\"dmri\" in s.protocol_name and \"b0\" in s.series_description and \"PA\" in s.series_description and s.dim4 == 1):\n            info[dwi_sbref].append({\"item\": s.series_id, \"acq\": \"b0\", 'dir': \"PA\"})\n        if (\"dmri\" in s.protocol_name and \"b0\" in s.series_description and \"PA\" in s.series_description and s.dim4 == 7):\n            info[dwi].append({\"item\": s.series_id, \"acq\": \"b0\", 'dir': \"PA\"})\n    return info\n```\n\n## step3_heudiconv.sh\n\n``` {.bash filename=\"step3_heudiconv.sh\" code-line-numbers=\"true\" code-fold=\"true\"}\n#!/bin/bash\n\n###############################\n### Original script from Arnaud Fournel, PhD, NeuroPop team, CRNL, Lyon\n### arnaud.fournel @ inserm.fr\n### \n### Adapted for the CRNL study\n### by Gaelle Leroux, PhD\n### and Isabelle Faillenot, PhD\n###\n### Autumn 2020, Lyon\n### gaelle.leroux @ cnrs.fr\n###\n### launched by: sbatch step3_heudiconv.sh ${subject} ${session_name} ${WD} ${my_study}\n###\n###############################\n#\n### The SBATCH directives (line 39 to be revised only):\n\n### Your job name displayed by the queue\n### use \"squeue\" command in a terminal to see it\n#SBATCH --job-name=HeuDC_3\n\n### Specify output and error files\n### %A for job array's master job allocation number.\n### or %a for job array ID (index) number\n#SBATCH --output=out_step3_%A.log\n#SBATCH --error=err_step3_%A.log\n\n### Specify the number of tasks, CPU per task and buffer size to be used\n### (up to 4 CPU/task to reach optimal power)\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=10G\n\n### Send email for which step: NONE, BEGIN, END, FAIL, REQUEUE, ALL\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=${firstName}.${lastName}@univ-lyon1.fr\n\n### The size of the participant table to be run\n### SBATCH --array=1-1\n\n### If you want to launch your script on a specific node of the cluser\n### If yes, uncomment the appropirate line(s) \n###SBATCH --exclude=node9\n###SBATCH --nodelist=node10\n\n# End of the SBATCH directives\n###############################\n#\n# Paths of the study\nsubject=$1\nsession_name=$2\nWD=$3\nmy_study=$4\nSTUDY=${WD}/${my_study}\nDATA_DIRECTORY=\"${STUDY}/data\"\nrm ${STUDY}/data/bids/${subject}/${session_name}/.heudiconv/*.edit.txt\n#\nHEUDICONV_SINGULARITY_IMG=\"/mnt/data/soft/Images/heudiconv_1.1.0.sif\"\n#\n## Comment: Parse the participants.tsv file and extract one subject ID from the line corresponding to this SLURM task.\n## Comment: 1 line below to uncomment if you want to launch the script for all the subjects listed in ${DATA_DIRECTORY}/participants.tsv\n#subject=$( sed -n -E \"$((${SLURM_ARRAY_TASK_ID} + 1))s/sub-(\\S*)\\>.*/\\1/gp\" ${DATA_DIRECTORY}/participants.tsv )\n#\n# To be printed in the out_*.log file :\necho \"#########################################################################\"\necho \"User:\" $USER\necho \"#\"\necho \"SLURM_SUBMITING_DIRECTORY:\" $SLURM_SUBMIT_DIR\necho \"SLURM_JOB_NODELIST:\" $SLURM_NODELIST\necho \"SLURM_JOB_NAME:\" $SLURM_JOB_NAME\necho \"SLURM_JOB_ID:\" $SLURM_JOBID\necho \"SLURM_ARRAY_TASK_ID:\" $SLURM_ARRAY_TASK_ID\necho \"SLURM_NTASKS:\" $SLURM_NTASKS\necho \"#\"\necho \"Step 3: conversion of DICOM to NIFTI with BIDS standards using HeuDiConv\"\necho \"#\"\necho \"Subject processed:\" ${subject}\necho \"Session processed:\" ${session_name}\necho \"#\"\necho \"Job STARTED @ $(date)\"heuristic.py\necho \"#\"\n#\n# STEP 3/3: conversion of DICOM to NIFTII using dcm2niix and to a BIDS standard organisation\n# submission to slurm and running the HeuDiCon singularity image\n#\n# Check the \"-d\" path (line 87) pointing at the dicom files\n#\n# Compose the command line\ncmd=\"srun singularity run \\\n    --cleanenv \\\n    -B ${DATA_DIRECTORY}:/base -B ${STUDY}:/study \\\n    ${HEUDICONV_SINGULARITY_IMG} \\\n    -d /base/dicom/{subject}/{session}/scans/*/resources/DICOM/files/*.??? \\\n    -s ${subject} \\\n    --ses ${session_name} \\\n    -f /study/code/heuristic.py \\\n    -c dcm2niix -b \\\n    -o /base/bids \\\n    --minmeta \\\n    --overwrite\"\n#\n# Setup done, run the command\necho Running task ${SLURM_ARRAY_TASK_ID}\necho \"Command line used (example of the last subject processed)\"\necho $cmd\n#\necho \"#\"\n#\neval $cmd\nexitcode=$?\n#\necho \"#\"\necho \"Job STOPPED @ $(date)\"\necho \"#\"\necho \"If STEP 3 successful: check the text files & folders (.heudiconv and sub-{$1}) in\" \necho ${DATA_DIRECTORY}\"/bids\"\necho \"Edit the file\" ${DATA_DIRECTORY}\"/dataset_description.json\"\necho \"Create events.tsv file for each func/*.json file\"\necho \"#\"\necho \"Then, validate your nifti folder using online BIDS VALIDATOR: https://bids-standard.github.io/bids-validator/\"\necho \"############################################################################################\"\necho \"#\"\n# Output results to a table\necho \"sub-${subject}    ${SLURM_ARRAY_TASK_ID}  $exitcode\" >> ${SLURM_JOB_NAME}.step3.${SLURM_ARRAY_JOB_ID}.tsv\necho Finished tasks ${SLURM_ARRAY_TASK_ID} with exit code $exitcode\nexit $exitcode\n```\n","srcMarkdownNoYaml":"\n\n# Introduction\n\nThis document is a step-by-step guide on how to convert raw data from a study to [BIDS](https://bids.neuroimaging.io/) format.[^1] The tool that is used here is [HeuDiConv](https://neuroimaging-core-docs.readthedocs.io/en/latest/pages/heudiconv.html). The data used in this example is from the Tiger study and already downloaded from xnat (see - add link when right this thing). The data are in a dicom format. Running HeuDiConv is a 3 step procedure. Note that this procedure takes place to the crnl cluster so this code is to be used with the cluster terminal and slurm. Most of the code is written in bash, but there are some python scripts that needs to be modified manually for the Bids conversion.\n\n[^1]: For an excellent tutorial for Bids conversion [see](https://sarenseeley.github.io/BIDS-fmriprep-MRIQC.html).\n\n# Bids Conversion\n\n## Setup\n\nIn this preliminary step we setup and define our variables. Note from the code below, change the variable \"subject\" to the subject you are working on. The Tiger notation is C for controls and G for Gamblers, e.g. for the first control subject the notation is C01 and for the first gambler G01. The following procedure is to submit the job for one subject. (to be updated for more).\n\n``` bash\n# Standard variable for the Tiger study\nWD=/crnldata/psyr2/Anastasios/Tiger_fmri\nmy_study=Tiger\nsession_name=S01\n\n# change this to the subject you are working on\nsubject=XXX \n```\n\n## [Step 1](https://neuroimaging-core-docs.readthedocs.io/en/latest/pages/heudiconv.html#heudiconv-step1): Generate a heuristic.py file.\n\nBy passing some path information and flags to HeuDiConv, you generate a heuristic (translation) file skeleton and some associated descriptor text files. These all get placed in a **hidden** directory, .heudiconv under the bids/derivatives directory.\n\n``` bash\ncd ${WD}/${my_study}/code\nsbatch step1_heudiconv.sh ${subject} ${session_name} ${WD} ${my_study} \nsqueue\n```\n\nTo check that it worked the out_step1\\_\\*.log file should have finished with exit code 0. Also, go to the /crnldata/psyr2/Anastasios/Tiger_fmri/Tiger/data/bids/derivatives/.heudiconv/{subject}/info were 5 files should be created.\n\nNote that this is a **hidden** directory. To see it you need to type `ls -a` in the terminal. The file that we need for the next step is the dicominfo.tsv file.\n\nThis step takes around 15 to 20 minutes to finish.\n\n## [Step 2](https://neuroimaging-core-docs.readthedocs.io/en/latest/pages/heudiconv.html#heudiconv-step2): Modify the heuristic.py file\n\nYou will modify the heuristic.py to specify BIDS output names and directories, and the input DICOM characteristics. Available input DICOM characteristics are listed in /.heudiconv/info/dicominfo.tsv.\n\nCheck the template of the heuristic.py file specifically for the Tiger study in the following path: /crnldata/psyr2/Anastasios/Tiger_fmri/Tiger/code/heuristic.py. This is the one you should modify based on the dicominfo.tsv file of the specific subject.\n\nThe template is almost ready. What you should modify are the following lines:\n\n1.  Line 119 s.dim == XXXX\n2.  Line 123 s.dim == XXXX\n3.  Line 127 s.dim == XXXX\n\nThe rest should remain the shame. At the end of this page there is the the heuristic.py file for inspection if nececarry.\n\n## [Step 3](https://neuroimaging-core-docs.readthedocs.io/en/latest/pages/heudiconv.html#heudiconv-step3): Run HeuDiConv\n\nNow that the heuristic.py file is ready we can run the HeuDiConv. Each time you run it, additional subdirectories are created under .heudiconv that record the details of each subject (and session) conversion. Detailed provenance information is retained in the .heudiconv hidden directory. The following code is to be run in the terminal.\n\n``` bash\ncd ${WD}/${my_study}/code\nchmod -R 770 .\nsbatch step3_heudiconv.sh ${subject} ${session_name} ${WD} ${my_study}\nsqueue\n```\n\n# Bids Validation\n\nAfter the conversion is done, it is important to validate the Bids format. This is done by using the [BIDS validator](https://bids-standard.github.io/bids-validator/).\n\n# Scripts\n\n## step1_heudiconv.sh\n\n``` {.bash filename=\"step1_heudiconv.sh\" code-line-numbers=\"true\" code-fold=\"true\"}\n#!/bin/bash\n\n###############################\n### Original script from Arnaud Fournel, PhD, NeuroPop team, CRNL, Lyon\n### arnaud.fournel @ inserm.fr\n### \n### Adapted for the CRNL study\n### by Gaelle Leroux, PhD\n### and Isabelle Faillenot, PhD\n###\n### Autumn 2020, Lyon\n### gaelle.leroux @ cnrs.fr\n###\n### launched by: sbatch step1_heudiconv.sh ${subject} ${session_name} ${WD} ${my_study}\n###\n###############################\n#\n### The SBATCH directives (line 39 to be revised only):\n\n### Your job name displayed by the queue\n### use \"squeue\" command in a terminal to see it\n#SBATCH --job-name=HeuDC_1\n\n### Specify output and error files\n### %A for job array's master job allocation number\n### or %a for job array ID (index) number\n#SBATCH --output=out_step1_%A.log\n#SBATCH --error=err_step1_%A.log\n\n### Specify the number of tasks, CPU per task and buffer size to be used \n### (up to 4 CPU/task to reach optimal power)\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=4\n#SBATCH --mem=10G\n\n### The size of the participant table to be run\n###SBATCH --array=1-1\n\n### If you want to launch your script on a specific node of the cluser\n### If yes, uncomment the appropirate line(s) \n###SBATCH --exclude=node9\n###SBATCH --nodelist=node10\n\n# End of the SBATCH directives\n###############################\n#\n# Paths of the study\nsubject=$1\nsession_name=$2\nWD=$3\nmy_study=$4\nSTUDY=${WD}/${my_study}\nDATA_DIRECTORY=\"${STUDY}/data\"\n#\nHEUDICONV_SINGULARITY_IMG=\"/mnt/data/soft/Images/heudiconv_1.1.0.sif\"\n#\n## Comment: Parse the participants.tsv file and extract one subject ID from the line corresponding to this SLURM task.\n## Comment: 1 line below to uncomment if you want to launch the script for all the subjects listed in ${DATA_DIRECTORY}/participants.tsv\n#subject=$( sed -n -E \"$((${SLURM_ARRAY_TASK_ID} + 1))s/sub-(\\S*)\\>.*/\\1/gp\" ${DATA_DIRECTORY}/participants.tsv )\n#\n# To be printed in the out_*.log file :\necho \"#########################################################################\"\necho \"User:\" $USER\necho \"#\"\necho \"SLURM_SUBMITING_DIRECTORY:\" $SLURM_SUBMIT_DIR\necho \"SLURM_JOB_NODELIST:\" $SLURM_NODELIST\necho \"SLURM_JOB_NAME:\" $SLURM_JOB_NAME\necho \"SLURM_JOB_ID:\" $SLURM_JOBID\necho \"SLURM_ARRAY_TASK_ID:\" $SLURM_ARRAY_TASK_ID\necho \"SLURM_NTASKS:\" $SLURM_NTASKS\necho \"#\"\necho \"Step 1: generation of text files using HeuDiConv\"\necho \"#\"\necho \"Subject processed:\" ${subject}\necho \"Session processed:\" ${session_name}\necho \"#\"\necho \"Job STARTED @ $(date)\"\necho \"#\"\n#\n# STEP 1/3: generate heuristic file based on a template file + 4 other text files\n# Submission to slurm and running the HeuDiConv singularity image\n#\n# Check the \"-d\" path (line 83) pointing at the dicom files\n#\n# Compose the command line\ncmd=\"srun singularity run \\\n    --cleanenv \\\n    -B ${DATA_DIRECTORY}:/base \\\n    ${HEUDICONV_SINGULARITY_IMG} \\\n    -d /base/dicom/{subject}/{session}/scans/*/resources/DICOM/files/*.??? \\\n    -s ${subject} \\\n    --ses ${session_name} \\\n    -f convertall \\\n    -c none \\\n    -o /base/bids/derivatives \\\n    --overwrite\"\n#\n# Setup done, run the command\necho Running task ${SLURM_ARRAY_TASK_ID}\necho \"Command line used (example of the last subject processed)\"\necho $cmd\n#\necho \"#\"\n#\neval $cmd\nexitcode=$?\n#\necho \"#\"\necho \"Job STOPPED @ $(date)\"\necho \"#\"\necho \"If STEP 1 successful: 5 text files were generated in\" ${STUDY}\"/data/bids/derivatives/.heudiconv/\"$1\"/info\"\necho \"#\"\necho \"Now, STEP 2: edit the empty file heuristitic.py just created and copy/paste it to\" ${STUDY}\"/code\"\necho \"#\"\necho \"An example of heuristic file for a study@primage is provided in\" ${STUDY}\"/code/heuristic_templates/heuristic.py\"\necho \"#########################################################################\"\necho \"#\"\n# For your information about step 2:\n# The heuristic file controls how information about the dicoms is used to convert to a file system layout (e.g., BIDS). \n# This is a python file that must have the function infotodict, which takes a single argument seqinfo.\n#\n# Output results to a table\necho \"sub-$subject  ${SLURM_ARRAY_TASK_ID} $exitcode\" >> ${SLURM_JOB_NAME}.step1.${SLURM_ARRAY_JOB_ID}.tsv\necho Finished tasks ${SLURM_ARRAY_TASK_ID} with exit code $exitcode\nexit $exitcode\n```\n\n## heuristic.py\n\n``` {.python filename=\"heuristic.py\" code-line-numbers=\"true\" code-fold=\"true\"}\nfrom __future__ import annotations\n\nimport logging\nfrom typing import Optional\n\nfrom heudiconv.utils import SeqInfo\n\nlgr = logging.getLogger(\"heudiconv\")\n\n\ndef create_key(\n    template: Optional[str],\n    outtype: tuple[str, ...] = (\"nii.gz\",),\n    annotation_classes: None = None,\n) -> tuple[str, tuple[str, ...], None]:\n    if template is None or not template:\n        raise ValueError(\"Template must be a valid format string\")\n    return (template, outtype, annotation_classes)\n\n\ndef infotodict(\n    seqinfo: list[SeqInfo],\n) -> dict[tuple[str, tuple[str, ...], None], list[str]]:\n    \"\"\"Heuristic evaluator for determining which runs belong where\n\n    allowed template fields - follow python string module:\n\n    item: index within category\n    subject: participant id\n    seqitem: run number during scanning\n    subindex: sub index within group\n    \"\"\"\n\n    # \"data\" creates sequential numbers which can be for naming sequences.\n    # This is especially valuable if you run the same sequence multiple times at the scanner.\n    data = create_key('run-{item:03d}')\n\n    # Anatomical images\n    # Structural scans (anat specification): MUST end with \"T1w\" or \"T2w\" or \"FLAIR\" or \"T1map\"...\n    # list: https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/01-magnetic-resonance-imaging-data.html#anatomy-imaging-data\n    t1w = create_key('sub-{subject}/{session}/anat/sub-{subject}_{session}_T1w')\n\n    # Functional images\n    # Tasks, including movies (func specification): MUST contain \"task-\" in the name + \"bold\" or \"sbref\" or \"cbv\" or \"phase\" at the end\n    # list: https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/01-magnetic-resonance-imaging-data.html#task-including-resting-state-imaging-data\n    # For tasks and movies, we need to specify the task name in the file name and then all the tasks are in the func folder. \n\n    # Big Buck Bunny task\n    boldBigBuckBunny = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-BigBuckBunny_bold')\n    boldBigBuckBunny_sbref = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-BigBuckBunny_sbref')\n\n    # Hariri task\n    boldHariri = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-Hariri_bold')\n    boldHariri_sbref = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-Hariri_sbref')\n\n    # Partly Cloudy task\n    boldPartlyCloudy = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-PartlyCloudy_bold')\n    boldPartlyCloudy_sbref = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-PartlyCloudy_sbref')   \n\n    # Skewed gambling task NOTE: this task has multiple runs\n    boldSkewedGambling = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-SkewedGambling_run-{item:02d}_bold')\n    boldSkewedGambling_sbref = create_key('sub-{subject}/{session}/func/sub-{subject}_{session}_task-SkewedGambling_run-{item:02d}_sbref')\n\n\n    # field maps (fmap specification): the file name must end with \"magnitude\" or \"phasediff\" and include the {subject}\n    # specifications: https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/01-magnetic-resonance-imaging-data.html#fieldmap-data\n    fmap_magn = create_key('sub-{subject}/{session}/fmap/sub-{subject}_{session}_magnitude')\n    fmap_phase = create_key('sub-{subject}/{session}/fmap/sub-{subject}_{session}_phasediff')\n\n    # Diffusion scans (dwi speccification): MUST end with \"dwi\"\n    # specifications: https://bids-specification.readthedocs.io/en/stable/04-modality-specific-files/01-magnetic-resonance-imaging-data.html#diffusion-imaging-data\n    dwi = create_key('sub-{subject}/{session}/dwi/sub-{subject}_{session}_acq-{acq}_dir-{dir}_dwi')\n    dwi_sbref = create_key('sub-{subject}/{session}/dwi/sub-{subject}_{session}_acq-{acq}_dir-{dir}_sbref')\n\n\n    info: dict[tuple[str, tuple[str, ...], None], list[str]] = {data: [],\n                                                                boldHariri: [],\n                                                                boldHariri_sbref: [],\n                                                                boldBigBuckBunny: [],\n                                                                boldBigBuckBunny_sbref: [],\n                                                                boldPartlyCloudy: [],\n                                                                boldPartlyCloudy_sbref: [],\n                                                                boldSkewedGambling: [],\n                                                                boldSkewedGambling_sbref: [],\n                                                                fmap_magn: [],\n                                                                fmap_phase: [],\n                                                                t1w: [],\n                                                                dwi: [],\n                                                                dwi_sbref: []\n                                                                }\n\n    for s in seqinfo:\n        \"\"\"\n        The namedtuple `s` contains the following fields:\n\n        * total_files_till_now\n        * example_dcm_file\n        * series_id\n        * dcm_dir_name\n        * unspecified2\n        * unspecified3\n        * dim1\n        * dim2\n        * dim3\n        * dim4\n        * TR\n        * TE\n        * protocol_name\n        * is_motion_corrected\n        * is_derived\n        * patient_id\n        * study_description\n        * referring_physician_name\n        * series_description\n        * image_type\n        \"\"\"\n        if (\"emotion\" in s.protocol_name and s.dim4 == 3) :\n            info[boldHariri_sbref].append(s.series_id)\n        if (\"emotion\" in s.protocol_name and s.dim4 == 1257) :\n            info[boldHariri].append(s.series_id)\n        if (\"film_1\" in s.protocol_name and s.dim4 == 3) :\n            info[boldBigBuckBunny_sbref].append(s.series_id)\n        if (\"film_1\" in s.protocol_name and s.dim4 == 999) :\n            info[boldBigBuckBunny].append(s.series_id)\n        if (\"film_2\" in s.protocol_name and s.dim4 == 3) :\n            info[boldPartlyCloudy_sbref].append(s.series_id)\n        if (\"film_2\" in s.protocol_name and s.dim4 == 726) :\n            info[boldPartlyCloudy].append(s.series_id)\n        if (\"gambling\" in s.protocol_name and s.dim4 == 3) :\n            info[boldSkewedGambling_sbref].append(s.series_id)\n        if (\"gambling\" in s.protocol_name and s.dim4 > 1400 ) :\n            info[boldSkewedGambling].append(s.series_id)\n        if (\"gre_field_mapping\" in s.protocol_name and s.dim4 == 106):\n            info[fmap_magn].append(s.series_id)\n        if (\"gre_field_mapping\" in s.protocol_name and s.dim4 == 53):\n            info[fmap_phase].append(s.series_id)\n        if (\"T1_SAG\" in s.protocol_name) and ('NORM' in s.image_type) :\n            info[t1w].append(s.series_id)\n        if (\"dmri\" in s.protocol_name and \"b2700\" in s.series_description and \"PA\" in s.series_description and s.dim4 == 1):\n            info[dwi_sbref].append({\"item\": s.series_id, \"acq\": \"b2700\", 'dir': \"PA\"})\n        if (\"dmri\" in s.protocol_name and \"b2700\" in s.series_description and \"PA\" in s.series_description and s.dim4 == 151):\n            info[dwi].append({\"item\": s.series_id, \"acq\": \"b2700\", 'dir': \"PA\"})\n        if (\"dmri\" in s.protocol_name and \"b2700\" in s.series_description and \"AP\" in s.series_description and s.dim4 == 1):\n            info[dwi_sbref].append({\"item\": s.series_id, \"acq\": \"b2700\", 'dir': \"AP\"})\n        if (\"dmri\" in s.protocol_name and \"b2700\" in s.series_description and \"AP\" in s.series_description and s.dim4 == 151):\n            info[dwi].append({\"item\": s.series_id, \"acq\": \"b2700\", 'dir': \"AP\"})\n        if (\"dmri\" in s.protocol_name and \"b0\" in s.series_description and \"AP\" in s.series_description and s.dim4 == 1):\n            info[dwi_sbref].append({\"item\": s.series_id, \"acq\": \"b0\", 'dir': \"AP\"})\n        if (\"dmri\" in s.protocol_name and \"b0\" in s.series_description and \"AP\" in s.series_description and s.dim4 == 7):\n            info[dwi].append({\"item\": s.series_id, \"acq\": \"b0\", 'dir': \"AP\"})\n        if (\"dmri\" in s.protocol_name and \"b0\" in s.series_description and \"PA\" in s.series_description and s.dim4 == 1):\n            info[dwi_sbref].append({\"item\": s.series_id, \"acq\": \"b0\", 'dir': \"PA\"})\n        if (\"dmri\" in s.protocol_name and \"b0\" in s.series_description and \"PA\" in s.series_description and s.dim4 == 7):\n            info[dwi].append({\"item\": s.series_id, \"acq\": \"b0\", 'dir': \"PA\"})\n    return info\n```\n\n## step3_heudiconv.sh\n\n``` {.bash filename=\"step3_heudiconv.sh\" code-line-numbers=\"true\" code-fold=\"true\"}\n#!/bin/bash\n\n###############################\n### Original script from Arnaud Fournel, PhD, NeuroPop team, CRNL, Lyon\n### arnaud.fournel @ inserm.fr\n### \n### Adapted for the CRNL study\n### by Gaelle Leroux, PhD\n### and Isabelle Faillenot, PhD\n###\n### Autumn 2020, Lyon\n### gaelle.leroux @ cnrs.fr\n###\n### launched by: sbatch step3_heudiconv.sh ${subject} ${session_name} ${WD} ${my_study}\n###\n###############################\n#\n### The SBATCH directives (line 39 to be revised only):\n\n### Your job name displayed by the queue\n### use \"squeue\" command in a terminal to see it\n#SBATCH --job-name=HeuDC_3\n\n### Specify output and error files\n### %A for job array's master job allocation number.\n### or %a for job array ID (index) number\n#SBATCH --output=out_step3_%A.log\n#SBATCH --error=err_step3_%A.log\n\n### Specify the number of tasks, CPU per task and buffer size to be used\n### (up to 4 CPU/task to reach optimal power)\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=8\n#SBATCH --mem=10G\n\n### Send email for which step: NONE, BEGIN, END, FAIL, REQUEUE, ALL\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=${firstName}.${lastName}@univ-lyon1.fr\n\n### The size of the participant table to be run\n### SBATCH --array=1-1\n\n### If you want to launch your script on a specific node of the cluser\n### If yes, uncomment the appropirate line(s) \n###SBATCH --exclude=node9\n###SBATCH --nodelist=node10\n\n# End of the SBATCH directives\n###############################\n#\n# Paths of the study\nsubject=$1\nsession_name=$2\nWD=$3\nmy_study=$4\nSTUDY=${WD}/${my_study}\nDATA_DIRECTORY=\"${STUDY}/data\"\nrm ${STUDY}/data/bids/${subject}/${session_name}/.heudiconv/*.edit.txt\n#\nHEUDICONV_SINGULARITY_IMG=\"/mnt/data/soft/Images/heudiconv_1.1.0.sif\"\n#\n## Comment: Parse the participants.tsv file and extract one subject ID from the line corresponding to this SLURM task.\n## Comment: 1 line below to uncomment if you want to launch the script for all the subjects listed in ${DATA_DIRECTORY}/participants.tsv\n#subject=$( sed -n -E \"$((${SLURM_ARRAY_TASK_ID} + 1))s/sub-(\\S*)\\>.*/\\1/gp\" ${DATA_DIRECTORY}/participants.tsv )\n#\n# To be printed in the out_*.log file :\necho \"#########################################################################\"\necho \"User:\" $USER\necho \"#\"\necho \"SLURM_SUBMITING_DIRECTORY:\" $SLURM_SUBMIT_DIR\necho \"SLURM_JOB_NODELIST:\" $SLURM_NODELIST\necho \"SLURM_JOB_NAME:\" $SLURM_JOB_NAME\necho \"SLURM_JOB_ID:\" $SLURM_JOBID\necho \"SLURM_ARRAY_TASK_ID:\" $SLURM_ARRAY_TASK_ID\necho \"SLURM_NTASKS:\" $SLURM_NTASKS\necho \"#\"\necho \"Step 3: conversion of DICOM to NIFTI with BIDS standards using HeuDiConv\"\necho \"#\"\necho \"Subject processed:\" ${subject}\necho \"Session processed:\" ${session_name}\necho \"#\"\necho \"Job STARTED @ $(date)\"heuristic.py\necho \"#\"\n#\n# STEP 3/3: conversion of DICOM to NIFTII using dcm2niix and to a BIDS standard organisation\n# submission to slurm and running the HeuDiCon singularity image\n#\n# Check the \"-d\" path (line 87) pointing at the dicom files\n#\n# Compose the command line\ncmd=\"srun singularity run \\\n    --cleanenv \\\n    -B ${DATA_DIRECTORY}:/base -B ${STUDY}:/study \\\n    ${HEUDICONV_SINGULARITY_IMG} \\\n    -d /base/dicom/{subject}/{session}/scans/*/resources/DICOM/files/*.??? \\\n    -s ${subject} \\\n    --ses ${session_name} \\\n    -f /study/code/heuristic.py \\\n    -c dcm2niix -b \\\n    -o /base/bids \\\n    --minmeta \\\n    --overwrite\"\n#\n# Setup done, run the command\necho Running task ${SLURM_ARRAY_TASK_ID}\necho \"Command line used (example of the last subject processed)\"\necho $cmd\n#\necho \"#\"\n#\neval $cmd\nexitcode=$?\n#\necho \"#\"\necho \"Job STOPPED @ $(date)\"\necho \"#\"\necho \"If STEP 3 successful: check the text files & folders (.heudiconv and sub-{$1}) in\" \necho ${DATA_DIRECTORY}\"/bids\"\necho \"Edit the file\" ${DATA_DIRECTORY}\"/dataset_description.json\"\necho \"Create events.tsv file for each func/*.json file\"\necho \"#\"\necho \"Then, validate your nifti folder using online BIDS VALIDATOR: https://bids-standard.github.io/bids-validator/\"\necho \"############################################################################################\"\necho \"#\"\n# Output results to a table\necho \"sub-${subject}    ${SLURM_ARRAY_TASK_ID}  $exitcode\" >> ${SLURM_JOB_NAME}.step3.${SLURM_ARRAY_JOB_ID}.tsv\necho Finished tasks ${SLURM_ARRAY_TASK_ID} with exit code $exitcode\nexit $exitcode\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"self-contained":true,"output-file":"Bids_conversion.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.5.57","theme":"cosmo","title":"Bids Conversion","author":"Anastasios Dadiotis","editor":"visual"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}